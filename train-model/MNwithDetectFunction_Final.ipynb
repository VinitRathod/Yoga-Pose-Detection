{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"RrrFXgeWtLeq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a30603a5-f644-4540-a7ae-aba991b60139","executionInfo":{"status":"ok","timestamp":1744137864168,"user_tz":360,"elapsed":109604,"user":{"displayName":"CS711 711 BETA","userId":"11837583817338767600"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m552.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hTensorFlow Version: 2.19.0\n","OpenCV Version: 4.11.0\n","NumPy Version: 2.0.2\n"]}],"source":["# ----------------------------\n","# Package Installation\n","# ----------------------------\n","# Install TensorFlow Hub for pre-trained models\n","!pip install tensorflow_hub --quiet  # Suppress installation logs\n","\n","# Install headless OpenCV for image processing (no GUI dependencies)\n","!pip install opencv-python-headless --quiet\n","\n","# ----------------------------\n","# Core Imports\n","# ----------------------------\n","# TensorFlow ecosystem\n","import tensorflow as tf                 # Main ML framework\n","import tensorflow_hub as hub            # Pre-trained model repository\n","\n","# Computer Vision\n","import cv2                              # Image processing\n","from google.colab.patches import cv2_imshow  # Colab-compatible image display\n","\n","# Data Handling\n","import numpy as np                      # Numerical operations\n","import json                             # JSON file processing\n","import os                               # File system operations\n","\n","# Google Colab Integration\n","from google.colab import drive          # Mount Google Drive\n","from google.colab import files          # File upload/download\n","\n","# Machine Learning Utilities\n","from sklearn.model_selection import train_test_split  # Data splitting\n","from sklearn.ensemble import RandomForestClassifier  # Our classifier\n","from sklearn.metrics import (          # Model evaluation\n","    accuracy_score,\n","    classification_report,\n","    confusion_matrix\n",")\n","\n","# Visualization\n","import matplotlib.pyplot as plt         # Plotting and visualization\n","import seaborn as sns                   # Enhanced visualizations\n","plt.style.use('ggplot')                 # Professional plotting style\n","\n","# ----------------------------\n","# Version Verification\n","# ----------------------------\n","print(f\"TensorFlow Version: {tf.__version__}\")\n","print(f\"OpenCV Version: {cv2.__version__}\")\n","print(f\"NumPy Version: {np.__version__}\")\n","\n","# ----------------------------\n","# Configuration Constants\n","# ----------------------------\n","# Recommended to define these once at startup\n","MODEL_URL = \"https://tfhub.dev/google/movenet/singlepose/lightning/4\"\n","INPUT_SIZE = 192  # Movenet input dimension"]},{"cell_type":"markdown","metadata":{"id":"dWTUFvNxvCRv"},"source":["## Img preprocessing\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"mlqzoFHCvF8Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744137883395,"user_tz":360,"elapsed":19228,"user":{"displayName":"CS711 711 BETA","userId":"11837583817338767600"}},"outputId":"a3adc4ef-ddf0-4cdc-bc02-df82fd352de0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounting Google Drive...\n","Mounted at /content/drive\n","Drive mounted successfully!\n","\n"]},{"output_type":"stream","name":"stderr","text":["Analyzing dataset: 100%|██████████| 15/15 [00:01<00:00,  7.54it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","========================================\n","       Yoga Pose Dataset Summary        \n","========================================\n","Pose Name                 |      Count\n","----------------------------------------\n","Cow Pose                  |         87\n","Crane Pose                |         77\n","Upward Facing Dog         |         70\n","Low Lunge Pose            |         64\n","Shoulder Pressing Pose    |         61\n","Half Lord of the Fishes Pose |         60\n","Happy Baby Pose           |         59\n","Handstand Pose            |         59\n","Wild Thing Pose           |         54\n","Half Moon Pose            |         52\n","Sleeping Vishnu Pose      |         43\n","Half Frog Pose            |         40\n","Cat-Cow Stretch Pose      |         40\n","Frog Pose                 |         39\n","Eight Limbed Pose         |         33\n","========================================\n","TOTAL POSES:                         15\n","TOTAL IMAGES:                       838\n","========================================\n","\n","Average images per class: 55.9\n","Most images: 87 (Cow Pose)\n","Least images: 33 (Eight Limbed Pose)\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import os\n","from tqdm import tqdm\n","from google.colab import drive\n","from collections import defaultdict\n","\n","# ----------------------------\n","# Google Drive Mounting\n","# ----------------------------\n","print(\"Mounting Google Drive...\")\n","drive.mount('/content/drive')  # Requires authorization\n","print(\"Drive mounted successfully!\\n\")\n","\n","# ----------------------------\n","# Configuration\n","# ----------------------------\n","# Define dataset path (modify as needed)\n","DATASET_PATH = \"/content/drive/MyDrive/Yoga-Pose-Dataset-V1\"\n","SUPPORTED_EXTENSIONS = ('.png', '.jpg', '.jpeg', '.webp')  # Add more if needed\n","\n","# ----------------------------\n","# Dataset Analysis Function\n","# ----------------------------\n","def analyze_dataset(dataset_path):\n","    \"\"\"\n","    Analyzes a yoga pose dataset directory structure and counts images per class.\n","\n","    Args:\n","        dataset_path (str): Path to the root dataset directory\n","\n","    Returns:\n","        tuple: (pose_counts, total_images, total_classes)\n","            - pose_counts: Dictionary of {pose_name: image_count}\n","            - total_images: Sum of all images\n","            - total_classes: Number of pose classes\n","    \"\"\"\n","    pose_counts = defaultdict(int)\n","    problematic_folders = []\n","\n","    # Validate dataset path exists\n","    if not os.path.exists(dataset_path):\n","        raise FileNotFoundError(f\"Dataset path not found: {dataset_path}\")\n","\n","    # Process each pose folder with progress bar\n","    for pose_folder in tqdm(os.listdir(dataset_path), desc=\"Analyzing dataset\"):\n","        folder_path = os.path.join(dataset_path, pose_folder)\n","\n","        # Skip non-directory items\n","        if not os.path.isdir(folder_path):\n","            continue\n","\n","        try:\n","            # Count valid image files\n","            valid_files = [\n","                f for f in os.listdir(folder_path)\n","                if f.lower().endswith(SUPPORTED_EXTENSIONS)\n","            ]\n","            pose_counts[pose_folder] = len(valid_files)\n","\n","            # Warn if folder is empty\n","            if len(valid_files) == 0:\n","                problematic_folders.append(pose_folder)\n","\n","        except Exception as e:\n","            print(f\"\\nError processing {pose_folder}: {str(e)}\")\n","            problematic_folders.append(pose_folder)\n","\n","    # Print warnings if any issues found\n","    if problematic_folders:\n","        print(\"\\n⚠️ Warning: Issues found in these folders:\")\n","        for folder in problematic_folders:\n","            print(f\" - {folder}\")\n","\n","    return pose_counts, sum(pose_counts.values()), len(pose_counts)\n","\n","# ----------------------------\n","# Main Execution\n","# ----------------------------\n","if __name__ == \"__main__\":\n","    try:\n","        # Run dataset analysis\n","        pose_counts, total_images, total_classes = analyze_dataset(DATASET_PATH)\n","\n","        # Display results\n","        print(\"\\n\" + \"=\"*40)\n","        print(\"Yoga Pose Dataset Summary\".center(40))\n","        print(\"=\"*40)\n","        print(f\"{'Pose Name':<25} | {'Count':>10}\")\n","        print(\"-\"*40)\n","\n","        # Sort by count (descending)\n","        for pose, count in sorted(pose_counts.items(), key=lambda x: x[1], reverse=True):\n","            print(f\"{pose:<25} | {count:>10}\")\n","\n","        print(\"=\"*40)\n","        print(f\"TOTAL POSES: {total_classes:>26}\")\n","        print(f\"TOTAL IMAGES: {total_images:>25}\")\n","        print(\"=\"*40)\n","\n","        # Basic statistics\n","        avg_images = total_images / total_classes if total_classes > 0 else 0\n","        print(f\"\\nAverage images per class: {avg_images:.1f}\")\n","        print(f\"Most images: {max(pose_counts.values())} ({max(pose_counts, key=pose_counts.get)})\")\n","        print(f\"Least images: {min(pose_counts.values())} ({min(pose_counts, key=pose_counts.get)})\")\n","\n","    except Exception as e:\n","        print(f\"\\n❌ Error in dataset analysis: {str(e)}\")"]},{"cell_type":"markdown","metadata":{"id":"X09fgNCWwRSv"},"source":["Image Augmentation"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"_1JXJFWXwUU-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744137889375,"user_tz":360,"elapsed":5979,"user":{"displayName":"CS711 711 BETA","userId":"11837583817338767600"}},"outputId":"d9743574-7a1c-4a94-b427-fbb3065a3dc8"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:05<00:00,  2.55it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Yoga Pose Image Counts:\n","------------------------------\n","Upward Facing Dog   : 474 images\n","Sleeping Vishnu Pose: 255 images\n","Half Moon Pose      : 330 images\n","Cow Pose            : 607 images\n","Crane Pose          : 527 images\n","Wild Thing Pose     : 343 images\n","Happy Baby Pose     : 383 images\n","Low Lunge Pose      : 429 images\n","Eight Limbed Pose   : 179 images\n","Handstand Pose      : 383 images\n","Half Frog Pose      : 231 images\n","Half Lord of the Fishes Pose: 391 images\n","Shoulder Pressing Pose: 469 images\n","Frog Pose           : 312 images\n","Cat-Cow Stretch Pose: 320 images\n","------------------------------\n","TOTAL POSES: 15\n","TOTAL IMAGES: 5633\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import os\n","from tqdm import tqdm  # Optional, for progress bar\n","\n","# Path to your dataset\n","dataset_path = \"/content/drive/MyDrive/Maulik DataSet\"  # Change this to your folder path\n","\n","# Dictionary to store counts\n","pose_counts = {}\n","\n","# Get counts for each pose folder\n","for pose_folder in tqdm(os.listdir(dataset_path)):\n","    folder_path = os.path.join(dataset_path, pose_folder)\n","\n","    # Skip if not a directory\n","    if not os.path.isdir(folder_path):\n","        continue\n","\n","    # Count image files (supports .jpg, .png, .jpeg)\n","    image_count = 0\n","    for file in os.listdir(folder_path):\n","        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n","            image_count += 1\n","\n","    pose_counts[pose_folder] = image_count\n","\n","# Display results\n","print(\"\\nYoga Pose Image Counts:\")\n","print(\"-\" * 30)\n","for pose, count in pose_counts.items():\n","    print(f\"{pose:20s}: {count} images\")\n","print(\"-\" * 30)\n","print(f\"TOTAL POSES: {len(pose_counts)}\")\n","print(f\"TOTAL IMAGES: {sum(pose_counts.values())}\")"]},{"cell_type":"markdown","metadata":{"id":"-AF6y5dhwaXL"},"source":["EDA"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"JMMPj0sqwbdf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744138192670,"user_tz":360,"elapsed":285958,"user":{"displayName":"CS711 711 BETA","userId":"11837583817338767600"}},"outputId":"85b0e03b-ce1f-4daf-a0e8-b17597f8a803"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [04:42<00:00, 18.82s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","=== YOGA POSE DATASET EDA REPORT ===\n","\n","1. Basic Statistics:\n","- Total poses: 15\n","- Total images: 838\n","- Avg images per pose: 55.9\n","- Most common pose: Cow Pose (87 images)\n","- Rarest pose: Eight Limbed Pose (33 images)\n","\n","2. Image Characteristics:\n","- Average dimensions: 427x342 px\n","- Size variability: 196.2 (width) ± 149.2 (height)\n","- Most common aspect ratio: 1.00\n","\n","3. Color Analysis:\n","- Overall mean intensity: 159.8\n","- Highest contrast pose: Cat-Cow Stretch Pose\n","- Darkest pose: Frog Pose\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n","<ipython-input-1-783d354774a4>:61: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  - Most common pose: {df_counts.idxmax()[0]} ({df_counts.max()[0]} images)\n","<ipython-input-1-783d354774a4>:62: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  - Rarest pose: {df_counts.idxmin()[0]} ({df_counts.min()[0]} images)\n"]}],"source":["!pip install opencv-python # Installing opencv-python in the current cell's runtime\n","\n","import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm\n","\n","# Configuration\n","dataset_path = \"/content/drive/MyDrive/Yoga-Pose-Dataset-V1\"\n","output_path = \"eda_results\"\n","os.makedirs(output_path, exist_ok=True)\n","\n","# 1. Data Distribution Analysis\n","pose_counts = {}\n","image_sizes = []\n","color_stats = []\n","\n","for pose_folder in tqdm(os.listdir(dataset_path)):\n","    folder_path = os.path.join(dataset_path, pose_folder)\n","    if not os.path.isdir(folder_path):\n","        continue\n","\n","    count = 0\n","    for file in os.listdir(folder_path):\n","        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n","            img_path = os.path.join(folder_path, file)\n","            img = cv2.imread(img_path)\n","\n","            # Collect image stats\n","            image_sizes.append(img.shape[:2])  # (height, width)\n","\n","            # Collect color stats (BGR format)\n","            color_stats.append({\n","                'pose': pose_folder,\n","                'mean': img.mean(),\n","                'std': img.std()\n","            })\n","            count += 1\n","\n","    pose_counts[pose_folder] = count\n","\n","# Convert to DataFrames\n","df_counts = pd.DataFrame.from_dict(pose_counts, orient='index', columns=['count'])\n","df_sizes = pd.DataFrame(image_sizes, columns=['height', 'width'])\n","# Calculate aspect ratio and add it as a new column\n","df_sizes['aspect_ratio'] = df_sizes['width'] / df_sizes['height'] # Calculating aspect ratio\n","df_colors = pd.DataFrame(color_stats)\n","\n","\n","# 3. Generate Report\n","report = f\"\"\"\n","=== YOGA POSE DATASET EDA REPORT ===\n","\n","1. Basic Statistics:\n","- Total poses: {len(df_counts)}\n","- Total images: {df_counts['count'].sum()}\n","- Avg images per pose: {df_counts['count'].mean():.1f}\n","- Most common pose: {df_counts.idxmax()[0]} ({df_counts.max()[0]} images)\n","- Rarest pose: {df_counts.idxmin()[0]} ({df_counts.min()[0]} images)\n","\n","2. Image Characteristics:\n","- Average dimensions: {int(df_sizes['width'].mean())}x{int(df_sizes['height'].mean())} px\n","- Size variability: {df_sizes['width'].std():.1f} (width) ± {df_sizes['height'].std():.1f} (height)\n","- Most common aspect ratio: {df_sizes['aspect_ratio'].mode()[0]:.2f}\n","\n","3. Color Analysis:\n","- Overall mean intensity: {df_colors['mean'].mean():.1f}\n","- Highest contrast pose: {df_colors.groupby('pose')['std'].mean().idxmax()}\n","- Darkest pose: {df_colors.groupby('pose')['mean'].mean().idxmin()}\n","\"\"\"\n","\n","print(report)\n","with open(os.path.join(output_path, 'eda_report.txt'), 'w') as f:\n","    f.write(report)"]},{"cell_type":"markdown","metadata":{"id":"a_DLlRkhpNTE"},"source":["# Define Global variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ZhSqIH8tPxs","executionInfo":{"status":"aborted","timestamp":1744137897246,"user_tz":360,"elapsed":142770,"user":{"displayName":"CS711 711 BETA","userId":"11837583817338767600"}}},"outputs":[],"source":["# Load the MoveNet model from TensorFlow Hub\n","# Model: MoveNet SinglePose Lightning (fast, suitable for real-time applications)\n","movenet = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n","\n","# Define the mapping of keypoints to body parts\n","# These are the 17 keypoints detected by the MoveNet model in order\n","keypoint_names = ['nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear', 'left_shoulder', 'right_shoulder',\n","                  'left_elbow', 'right_elbow', 'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n","                  'left_knee', 'right_knee', 'left_ankle', 'right_ankle']\n","\n","# Dictionary mapping keypoint indices to human-readable names\n","# Useful for labeling keypoints when visualizing or interpreting model output\n","keypoint_names_dir = {\n","        0: \"Nose\", 1: \"Left Eye\", 2: \"Right Eye\", 3: \"Left Ear\", 4: \"Right Ear\",\n","        5: \"Left Shoulder\", 6: \"Right Shoulder\", 7: \"Left Elbow\", 8: \"Right Elbow\",\n","        9: \"Left Wrist\", 10: \"Right Wrist\", 11: \"Left Hip\", 12: \"Right Hip\",\n","        13: \"Left Knee\", 14: \"Right Knee\", 15: \"Left Ankle\", 16: \"Right Ankle\"\n","    }\n","\n","# Define the connections between keypoints for drawing pose skeleton\n","# Each tuple represents a pair of keypoints to be connected by a line\n","# Used in visualization to illustrate body posture\n","connections = [(0, 1), (0, 2), (1, 3), (2, 4), (0, 5), (0, 6), (5, 7), (7, 9), (6, 8), (8, 10),\n","               (5, 6), (5, 11), (6, 12), (11, 12), (11, 13), (13, 15), (12, 14), (14, 16)]\n"]},{"cell_type":"markdown","metadata":{"id":"75tP7qd-pZJG"},"source":["# Convert Image to Keypoints"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2jDObQSotRds","executionInfo":{"status":"aborted","timestamp":1744137897250,"user_tz":360,"elapsed":142774,"user":{"displayName":"CS711 711 BETA","userId":"11837583817338767600"}}},"outputs":[],"source":["def detect_pose_static(image_path):\n","    \"\"\"\n","    Detects human pose keypoints in a static image using the MoveNet model.\n","\n","    Parameters:\n","        image_path (str): Path to the image file.\n","\n","    Returns:\n","        numpy.ndarray: A NumPy array containing the detected keypoints.\n","                       Shape: [1, 1, 17, 3] where:\n","                           - 1st dim = batch size (always 1),\n","                           - 2nd dim = person count (always 1 for single-pose model),\n","                           - 3rd dim = 17 keypoints,\n","                           - 4th dim = [y, x, confidence] for each keypoint.\n","    \"\"\"\n","    # Read the image from the given file path\n","    image = cv2.imread(image_path)\n","\n","    # Convert the image from BGR (OpenCV default) to RGB (TensorFlow expects RGB)\n","    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","    # Resize the image to 192x192 with padding (required input size for MoveNet)\n","    # Add a batch dimension (axis=0) since the model expects a batch of images\n","    image_resized = tf.image.resize_with_pad(tf.expand_dims(image_rgb, axis=0), 192, 192)\n","\n","    # Convert the image to NumPy array with int32 type as expected by the model\n","    image_np = image_resized.numpy().astype(np.int32)\n","\n","    # Run the MoveNet model and get the keypoints\n","    # 'serving_default' is the default signature for inference\n","    outputs = movenet.signatures[\"serving_default\"](tf.constant(image_np))\n","\n","    # Extract the keypoints from the model output\n","    keypoints = outputs['output_0'].numpy()\n","\n","    # Return the keypoints (shape: [1, 1, 17, 3] => batch, person, keypoints, [y, x, confidence])\n","    return keypoints\n"]},{"cell_type":"markdown","metadata":{"id":"exxyxgmjpds5"},"source":["# Calculate Angle Difference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jt5IZ6vItTre","executionInfo":{"status":"aborted","timestamp":1744137897252,"user_tz":360,"elapsed":142776,"user":{"displayName":"CS711 711 BETA","userId":"11837583817338767600"}}},"outputs":[],"source":["def calculate_angle(a, b, c):\n","    \"\"\"\n","    Calculates the angle (in degrees) formed at point 'b' by the line segments ab and bc.\n","\n","    Parameters:\n","        a (list or np.array): Coordinates of the first point (e.g., [x, y]).\n","        b (list or np.array): Coordinates of the middle joint (angle vertex).\n","        c (list or np.array): Coordinates of the third point (e.g., [x, y]).\n","\n","    Returns:\n","        float: The angle in degrees between the line segments ab and bc.\n","    \"\"\"\n","\n","    # Convert input points to NumPy arrays for vector operations\n","    a = np.array(a)  # First joint\n","    b = np.array(b)  # Middle joint (vertex)\n","    c = np.array(c)  # End joint\n","\n","    # Vectors from the middle joint to the other two\n","    ba = a - b\n","    bc = c - b\n","\n","    # Compute the cosine of the angle using the dot product formula\n","    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n","\n","    # Clip cosine to the valid domain of arccos to avoid numerical issues\n","    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n","\n","    # Convert the angle from radians to degrees\n","    return np.degrees(angle)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fIgPmLjmLOEb","executionInfo":{"status":"aborted","timestamp":1744137897270,"user_tz":360,"elapsed":142794,"user":{"displayName":"CS711 711 BETA","userId":"11837583817338767600"}}},"outputs":[],"source":["# Function to check if an angle is within an acceptable range\n","def is_within_tolerance(expected, actual, tolerance):\n","    \"\"\"\n","    Checks whether the actual angle is within the acceptable tolerance range of the expected angle.\n","\n","    Parameters:\n","        expected (float): The target angle in degrees.\n","        actual (float): The detected angle in degrees.\n","        tolerance (float): The allowed deviation from the expected angle.\n","\n","    Returns:\n","        bool: True if actual is within ±tolerance of expected, otherwise False.\n","    \"\"\"\n","    return abs(expected - actual) <= tolerance\n","\n","\n","# Tolerance levels based on difficulty (in degrees)\n","# Higher tolerance for beginners, stricter for advanced users\n","tolerance_levels = {\n","    \"Beginner\": 50,       # More forgiving (±50°)\n","    \"Intermediate\": 15,   # Moderate strictness (±15°)\n","    \"Advanced\": 10        # Strictest (±10°)\n","}\n","\n","\n","# Default tolerance range per joint for \"Advanced\" level\n","# These values reflect how precise a joint angle should be\n","default_tolerance_ranges = {\n","    \"Left Elbow\": 10, \"Right Elbow\": 10,\n","    \"Left Knee\": 15, \"Right Knee\": 15,\n","    \"Left Shoulder\": 12, \"Right Shoulder\": 12,\n","    \"Left Hip\": 18, \"Right Hip\": 18\n","}\n","\n","\n","# Function to update joint tolerance values based on difficulty level\n","def adjust_tolerance_levels(level=\"Advanced\"):\n","    \"\"\"\n","    Adjusts the tolerance values for joint angles based on the user's skill level.\n","\n","    Parameters:\n","        level (str): Difficulty level (\"Beginner\", \"Intermediate\", \"Advanced\").\n","\n","    Returns:\n","        dict: A dictionary with updated tolerance per joint, scaled according to difficulty.\n","    \"\"\"\n","    base_tolerance = tolerance_levels.get(level, 10)  # Default to Advanced if invalid level\n","    scale_factor = base_tolerance / 10  # Scale based on Advanced baseline (10)\n","\n","    # Scale each joint's default tolerance by the calculated factor\n","    return {joint: int(value * scale_factor) for joint, value in default_tolerance_ranges.items()}\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uaay4MS7tVG5","executionInfo":{"status":"aborted","timestamp":1744137897273,"user_tz":360,"elapsed":142797,"user":{"displayName":"CS711 711 BETA","userId":"11837583817338767600"}}},"outputs":[],"source":["def provide_correction_feedback(detected_keypoints, reference_keypoints, predicted_pose_name, skill_level=\"Intermediate\"):\n","    \"\"\"\n","    Provides feedback on pose accuracy by comparing detected keypoint angles to reference keypoints.\n","\n","    Parameters:\n","        detected_keypoints (list): List of 17 detected keypoints from the user [ [x, y], ... ].\n","        reference_keypoints (list): List of reference pose data dicts. Each dict should have:\n","                                    - \"pose\": pose name (str)\n","                                    - \"keypoints\": list with shape [1, 1, 17, 3]\n","        predicted_pose_name (str): The pose name predicted for the user's pose.\n","        skill_level (str): User's skill level (\"Beginner\", \"Intermediate\", \"Advanced\").\n","\n","    Returns:\n","        list: Feedback strings indicating corrections, or confirmation if pose is correct.\n","    \"\"\"\n","\n","    feedback = []\n","\n","    # Critical joints represented as tuples of (start, middle, end) indices\n","    critical_joints = [(5, 7, 9),   # Left arm\n","                       (6, 8, 10),  # Right arm\n","                       (11, 13, 15),  # Left leg\n","                       (12, 14, 16)]  # Right leg\n","\n","    # Get tolerance levels based on skill level\n","    default_tolerance = adjust_tolerance_levels(skill_level)\n","\n","    # Filter reference keypoints for the predicted pose\n","    matching_references = [ref for ref in reference_keypoints if ref[\"pose\"] == predicted_pose_name]\n","\n","    # Extract all keypoints from the matching references\n","    all_keypoints = [ref[\"keypoints\"] for ref in matching_references]\n","\n","    if not all_keypoints:\n","        return [\"Error: No reference keypoints found for the predicted pose.\"]\n","\n","    # Validate detected keypoints\n","    if len(detected_keypoints) < 17:\n","        return [\"Error: Invalid keypoint data.\"]\n","\n","    for joints in critical_joints:\n","        try:\n","            # Compute angle from detected user pose\n","            detected_angle = calculate_angle(\n","                detected_keypoints[joints[0]],\n","                detected_keypoints[joints[1]],\n","                detected_keypoints[joints[2]]\n","            )\n","\n","            # Compute average angle from all reference keypoints\n","            reference_angles = []\n","            for ref in all_keypoints:\n","                if len(ref[0][0]) < 17:\n","                    return [\"Error: Invalid reference keypoint data.\"]\n","                ref_kps = ref[0][0]\n","                angle = calculate_angle(ref_kps[joints[0]], ref_kps[joints[1]], ref_kps[joints[2]])\n","                reference_angles.append(angle)\n","\n","            reference_angle = sum(reference_angles) / len(reference_angles)\n","\n","            # Get the joint name and its specific tolerance\n","            joint_name = keypoint_names_dir[joints[1]]\n","            tolerance = default_tolerance.get(joint_name, 10)  # fallback tolerance\n","\n","            # Compare angles\n","            if not is_within_tolerance(reference_angle, detected_angle, tolerance):\n","                feedback.append(\n","                    f\"Adjust angle at {joint_name}: Expected {reference_angle:.2f}°, got {detected_angle:.2f}° (Tolerance: ±{tolerance}°).\"\n","                )\n","\n","        except (IndexError, KeyError, TypeError) as e:\n","            return [f\"Error: Missing or incorrect keypoints for joints {joints}. Exception: {str(e)}\"]\n","\n","    return feedback if feedback else [\"Pose is correct!\"]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qJUbDmigNC_w","executionInfo":{"status":"aborted","timestamp":1744137897276,"user_tz":360,"elapsed":142799,"user":{"displayName":"CS711 711 BETA","userId":"11837583817338767600"}}},"outputs":[],"source":["# Function to determine color based on deviation from expected angle\n","def get_color_based_on_tolerance(expected, actual, tolerance):\n","    \"\"\"\n","    Determines a color (BGR format) based on how close the actual angle is to the expected angle.\n","\n","    Parameters:\n","        expected (float): The reference or expected angle.\n","        actual (float): The detected angle from the user's pose.\n","        tolerance (float): The acceptable deviation from the expected angle.\n","\n","    Returns:\n","        tuple: A BGR color tuple for OpenCV drawing:\n","               - Green (0, 255, 0) if within tolerance\n","               - Yellow (0, 255, 255) if slightly off (<= 1.5x tolerance)\n","               - Red (0, 0, 255) if significantly off (> 1.5x tolerance)\n","    \"\"\"\n","    deviation = abs(expected - actual)\n","\n","    if deviation <= tolerance:\n","        return (0, 255, 0)  # Green = Correct\n","    elif deviation <= tolerance * 1.5:\n","        return (0, 255, 255)  # Yellow = Needs attention\n","    else:\n","        return (0, 0, 255)  # Red = Needs correction\n"]},{"cell_type":"markdown","metadata":{"id":"6So9KWC3p06a"},"source":["# Draw Keypoint in Upload Image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pm4gT00rtYNr","executionInfo":{"status":"aborted","timestamp":1744137897276,"user_tz":360,"elapsed":142799,"user":{"displayName":"CS711 711 BETA","userId":"11837583817338767600"}}},"outputs":[],"source":["def visualize_pose_static(image_path, keypoints, reference_keypoints=None, predicted_pose_name=\"\", skill_level=\"Intermediate\"):\n","    \"\"\"\n","    Visualizes the detected pose on the given static image and provides correction feedback if reference keypoints are provided.\n","\n","    Parameters:\n","        image_path (str): Path to the image file to be processed.\n","        keypoints (list or np.array): Detected keypoints in the format [ [x, y], ... ].\n","        reference_keypoints (list, optional): Reference keypoints for comparison to provide feedback on pose correctness.\n","        predicted_pose_name (str, optional): Name of the predicted pose to match with reference keypoints.\n","        skill_level (str, optional): Skill level for adjusting tolerance in corrections (\"Beginner\", \"Intermediate\", \"Advanced\").\n","\n","    Returns:\n","        None: The function will display the image with drawn keypoints and connections, and print correction feedback if necessary.\n","    \"\"\"\n","\n","    # Read the image\n","    image = cv2.imread(image_path)\n","    keypoints = np.array(keypoints)\n","\n","    # Adjust keypoints shape based on input format\n","    if len(keypoints.shape) == 4:\n","        keypoints = keypoints[0, 0]  # Extract if shape is (1,1,17,3)\n","    elif len(keypoints.shape) == 3:\n","        keypoints = keypoints[0]  # Extract if shape is (1,17,3)\n","\n","    # Replace NaN values with zeros\n","    keypoints = np.nan_to_num(keypoints)\n","\n","    height, width, _ = image.shape\n","\n","    # Default color for keypoints if no feedback is provided\n","    default_color = (0, 255, 0)  # Green\n","\n","    # Initialize feedback for pose correction\n","    feedback = []\n","    tolerance = 10  # Default angle tolerance\n","    default_tolerance = adjust_tolerance_levels(skill_level)  # Adjust based on skill level\n","\n","    # If reference keypoints are provided, calculate correction feedback\n","    if reference_keypoints is not None:\n","        feedback = provide_correction_feedback(keypoints, reference_keypoints, predicted_pose_name, skill_level)\n","\n","    # Draw keypoints\n","    for i, kp in enumerate(keypoints):\n","        x = int(kp[1] * width)\n","        y = int(kp[0] * height)\n","\n","        # Set color based on feedback, default to green\n","        color = default_color\n","\n","        if reference_keypoints is not None:\n","            # Check feedback and adjust color for the keypoint\n","            for msg in feedback:\n","                if keypoint_names_dir[i] in msg:\n","                    expected_angle = float(msg.split(\"Expected\")[1].split(\"°\")[0].strip())\n","                    actual_angle = float(msg.split(\"got\")[1].split(\"°\")[0].strip())\n","                    tolerance = default_tolerance.get(keypoint_names_dir[i], 10)\n","                    color = get_color_based_on_tolerance(expected_angle, actual_angle, tolerance)\n","\n","        # Draw the keypoint as a circle\n","        cv2.circle(image, (x, y), 12, color, -1)\n","\n","    # Draw connections between keypoints (limbs and torso)\n","    connections = [\n","        (5, 7), (7, 9), (6, 8), (8, 10),  # Arms\n","        (11, 13), (13, 15), (12, 14), (14, 16),  # Legs\n","        (5, 6), (11, 12), (5, 11), (6, 12)  # Torso\n","    ]\n","\n","    for connection in connections:\n","        idx1, idx2 = connection\n","        x1, y1 = int(keypoints[idx1, 1] * width), int(keypoints[idx1, 0] * height)\n","        x2, y2 = int(keypoints[idx2, 1] * width), int(keypoints[idx2, 0] * height)\n","\n","        # Set line color based on feedback\n","        line_color = default_color\n","\n","        if reference_keypoints is not None:\n","            for msg in feedback:\n","                if keypoint_names_dir[idx1] in msg or keypoint_names_dir[idx2] in msg:\n","                    expected_angle = float(msg.split(\"Expected\")[1].split(\"°\")[0].strip())\n","                    actual_angle = float(msg.split(\"got\")[1].split(\"°\")[0].strip())\n","                    tolerance = default_tolerance.get(keypoint_names_dir[idx1], 10)\n","                    line_color = get_color_based_on_tolerance(expected_angle, actual_angle, tolerance)\n","\n","        # Draw the line between keypoints\n","        cv2.line(image, (x1, y1), (x2, y2), line_color, 4)\n","\n","    # Display the image with drawn keypoints and connections\n","    cv2_imshow(image)  # Display in Colab\n","\n","    # Print corrections if necessary\n","    if feedback:\n","        print(\"\\nCorrections Needed:\")\n","        for msg in feedback:\n","            print(msg)\n"]},{"cell_type":"markdown","metadata":{"id":"hV9O-3yvp9nZ"},"source":["# Get Dataset and save Keypoint Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6whWybxvtIoj","executionInfo":{"status":"aborted","timestamp":1744137897277,"user_tz":360,"elapsed":142800,"user":{"displayName":"CS711 711 BETA","userId":"11837583817338767600"}}},"outputs":[],"source":["# Mount Google Drive to access the dataset\n","drive.mount('/content/drive')\n","\n","# Define the path to the dataset folder in Google Drive\n","dataset_path = \"/content/drive/MyDrive/Maulik DataSet\"\n","\n","# Initialize an empty list to store pose data (class and keypoints)\n","pose_data = []\n","\n","# Loop through each pose class folder in the dataset directory\n","for pose_class in os.listdir(dataset_path):\n","    # Get the full path to the current pose class folder\n","    pose_folder = os.path.join(dataset_path, pose_class)\n","\n","    # Skip if the item is not a directory\n","    if not os.path.isdir(pose_folder):\n","        continue\n","\n","    # Process each image file in the current pose class folder\n","    for img_file in os.listdir(pose_folder):\n","        # Get the full path to the current image\n","        img_path = os.path.join(pose_folder, img_file)\n","\n","        # Detect pose keypoints from the static image\n","        keypoints = detect_pose_static(img_path)\n","\n","        # Append the pose class and corresponding keypoints to the list\n","        pose_data.append({\"pose\": pose_class, \"keypoints\": keypoints.tolist()})\n","\n","# Define the output path for the JSON file containing all keypoints\n","json_path = \"/content/yoga_keypoints.json\"\n","\n","# Save the collected pose data to a JSON file\n","with open(json_path, \"w\") as f:\n","    json.dump(pose_data, f)\n","\n","# Print confirmation that keypoints were saved\n","print(f\"Keypoints saved to {json_path}\")\n","\n","# Verify the saved data by loading it back\n","with open(\"/content/yoga_keypoints.json\", \"r\") as f:\n","    data = json.load(f)\n","    print(f\"Loaded {len(data)} keypoints from {json_path}\")"]},{"cell_type":"markdown","metadata":{"id":"U9HIVQWbqLG0"},"source":["# Build the Model to Predict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eqTEsVoAfx5g","executionInfo":{"status":"aborted","timestamp":1744137897277,"user_tz":360,"elapsed":142800,"user":{"displayName":"CS711 711 BETA","userId":"11837583817338767600"}}},"outputs":[],"source":["import json\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","import numpy as np\n","\n","# Load keypoint data from JSON file\n","with open(\"/content/yoga_keypoints.json\", \"r\") as f:\n","    data = json.load(f)\n","\n","# Initialize empty lists for features (X) and labels (y)\n","X = []\n","y = []\n","\n","# Process each item in the loaded data\n","for item in data:\n","    # Convert keypoints to numpy array and flatten into 1D vector\n","    X.append(np.array(item[\"keypoints\"]).flatten())  # Flatten keypoints\n","    # Add corresponding pose label\n","    y.append(item[\"pose\"])\n","\n","# Split data into training and testing sets (80% train, 20% test)\n","# random_state ensures reproducibility of the split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y,\n","    test_size=0.2,\n","    random_state=42\n",")\n","\n","# Initialize Random Forest classifier with 100 decision trees\n","clf = RandomForestClassifier(n_estimators=100)\n","\n","# Train the classifier on the training data\n","clf.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"Negpa7o-VfCq"},"source":["# Evaluation Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PINx2tzP0Hd5","executionInfo":{"status":"aborted","timestamp":1744137897277,"user_tz":360,"elapsed":142800,"user":{"displayName":"CS711 711 BETA","userId":"11837583817338767600"}}},"outputs":[],"source":["from sklearn.metrics import classification_report, confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Get unique pose classes from test labels for consistent ordering\n","pose_classes = np.unique(y_test)\n","\n","# Generate predictions on the test set using trained classifier\n","y_pred = clf.predict(X_test)\n","\n","# Print detailed classification report\n","print(\"Classification Report:\")\n","print(classification_report(\n","    y_test,\n","    y_pred,\n","    target_names=pose_classes  # Use actual class names for readability\n","))\n","\n","# Generate and visualize confusion matrix\n","cm = confusion_matrix(y_test, y_pred)\n","\n","# Create a heatmap visualization of the confusion matrix\n","plt.figure(figsize=(10, 8))  # Set figure size for readability\n","sns.heatmap(\n","    cm,\n","    annot=True,            # Display values in each cell\n","    fmt='d',               # Format as integers\n","    cmap='Blues',          # Blue color gradient\n","    xticklabels=pose_classes,  # Use class names for x-axis\n","    yticklabels=pose_classes   # Use class names for y-axis\n",")\n","plt.title('Confusion Matrix')  # Add title\n","plt.xlabel('Predicted')       # Label x-axis\n","plt.ylabel('Actual')          # Label y-axis\n","plt.show()                   # Display the plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q-91jWniVcFr","executionInfo":{"status":"aborted","timestamp":1744137897278,"user_tz":360,"elapsed":142801,"user":{"displayName":"CS711 711 BETA","userId":"11837583817338767600"}}},"outputs":[],"source":["# Import required libraries\n","from sklearn.ensemble import RandomForestClassifier  # For Random Forest implementation\n","from sklearn.metrics import accuracy_score          # For calculating accuracy metrics\n","import matplotlib.pyplot as plt                    # For visualization\n","\n","# Define range of tree counts to evaluate (from 1 to 100 trees, in increments of 10)\n","n_estimators = list(range(1, 101, 10))  # [1, 11, 21, ..., 91]\n","\n","# Initialize lists to store performance metrics\n","train_accuracies = []  # Will store training accuracy for each tree count\n","test_accuracies = []   # Will store testing accuracy for each tree count\n","train_errors = []      # Will store training error (1 - accuracy) for each tree count\n","test_errors = []       # Will store testing error (1 - accuracy) for each tree count\n","\n","# Iterate through each tree count configuration\n","for n in n_estimators:\n","    # Initialize Random Forest classifier with current parameters\n","    clf = RandomForestClassifier(\n","        n_estimators=n,               # Current number of trees being evaluated\n","        max_depth=10,                 # Prevent overfitting by limiting tree depth\n","        min_samples_split=5,          # Minimum samples required to split a node\n","        min_samples_leaf=3,           # Minimum samples required at each leaf node\n","        max_features='sqrt',          # Number of features to consider at each split (√n_features)\n","        random_state=42               # Seed for reproducibility\n","    )\n","\n","    # Train the model on training data\n","    clf.fit(X_train, y_train)\n","\n","    # Calculate training and testing accuracy\n","    train_acc = accuracy_score(y_train, clf.predict(X_train))  # Accuracy on training set\n","    test_acc = accuracy_score(y_test, clf.predict(X_test))     # Accuracy on test set\n","\n","    # Store metrics\n","    train_accuracies.append(train_acc)\n","    test_accuracies.append(test_acc)\n","    train_errors.append(1 - train_acc)  # Convert accuracy to error rate\n","    test_errors.append(1 - test_acc)    # Convert accuracy to error rate\n","\n","    # Print current iteration results in formatted output\n","    print(f\"Trees: {n:3d} | Train Accuracy: {train_acc:.4f} | Test Accuracy: {test_acc:.4f}\")\n","\n","# ----------------------------\n","# Visualization 1: Accuracy Curve\n","# ----------------------------\n","plt.figure(figsize=(8, 6))  # Set figure size\n","\n","# Plot training accuracy (blue line)\n","plt.plot(n_estimators, train_accuracies,\n","         label=\"Training Accuracy\",  # Legend label\n","         linewidth=2.5,              # Line thickness\n","         color='#1f77b4')           # Matplotlib default blue\n","\n","# Plot testing accuracy (green line)\n","plt.plot(n_estimators, test_accuracies,\n","         label=\"Test Accuracy\",\n","         linewidth=2.5,\n","         color='#2ca02c')           # Matplotlib default green\n","\n","# Chart styling\n","plt.xlabel(\"Number of Trees\", fontsize=12)      # X-axis label\n","plt.ylabel(\"Accuracy\", fontsize=12)             # Y-axis label\n","plt.title(\"Random Forest Accuracy Curve\", fontsize=14, pad=15)  # Title with padding\n","plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)  # Dashed grid lines\n","plt.legend(loc='lower right', framealpha=1)     # Legend in lower right with solid background\n","\n","# Set dynamic y-axis limits based on min/max accuracy values with 2% padding\n","plt.ylim(min(min(train_accuracies), min(test_accuracies)) - 0.02,\n","         max(max(train_accuracies), max(test_accuracies)) + 0.02)\n","\n","plt.tight_layout()  # Adjust layout to prevent label clipping\n","plt.show()          # Display the plot\n","\n","# ----------------------------\n","# Visualization 2: Loss Curve\n","# ----------------------------\n","plt.figure(figsize=(8, 6))  # Set figure size\n","\n","# Plot training error (blue line)\n","plt.plot(n_estimators, train_errors,\n","         label=\"Training Loss\",\n","         color='#1f77b4')  # Consistent color scheme\n","\n","# Plot testing error (green line)\n","plt.plot(n_estimators, test_errors,\n","         label=\"Test Loss\",\n","         color='#2ca02c')\n","\n","# Chart styling\n","plt.xlabel(\"Number of Trees\", fontsize=12)\n","plt.ylabel(\"Loss (1 - Accuracy)\", fontsize=12)\n","plt.title(\"Random Forest Loss Curve\", fontsize=14)\n","plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)  # Same grid style as above\n","plt.legend(fontsize=12)  # Show legend with consistent font size\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nvMsT-9xgsj6","executionInfo":{"status":"aborted","timestamp":1744137897278,"user_tz":360,"elapsed":142800,"user":{"displayName":"CS711 711 BETA","userId":"11837583817338767600"}}},"outputs":[],"source":["def predict_pose(image_path):\n","    \"\"\"\n","    Predicts the yoga pose from an input image using a trained classifier.\n","\n","    Args:\n","        image_path (str): Path to the input image file\n","\n","    Returns:\n","        tuple:\n","            - str: Predicted pose class name\n","            - numpy.ndarray: Detected keypoints array\n","\n","    Process Flow:\n","        1. Detects body keypoints from the input image\n","        2. Flattens the keypoints for classifier input\n","        3. Makes prediction using pre-trained Random Forest\n","        4. Returns both prediction and raw keypoints\n","    \"\"\"\n","\n","    # Step 1: Detect body keypoints from image\n","    # Uses your custom pose detection function\n","    keypoints = detect_pose_static(image_path)\n","\n","    # Step 2: Prepare keypoints for classifier\n","    # Flatten the keypoints array and reshape for sklearn's predict()\n","    # reshape(1, -1) converts it to 2D array with 1 sample (required format)\n","    keypoints_flat = keypoints.flatten().reshape(1, -1)\n","\n","    # Step 3: Make prediction using trained classifier\n","    # clf should be your pre-trained RandomForestClassifier\n","    # [0] gets the first (and only) prediction from the 1D result array\n","    prediction = clf.predict(keypoints_flat)[0]\n","\n","    # Step 4: Return both prediction and original keypoints\n","    # Returning keypoints allows for visualization or further processing\n","    return prediction, keypoints"]},{"cell_type":"markdown","metadata":{"id":"y28DiEehB2dd"},"source":["# Getting Feedback From Json File"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tozxfnT7BwpO","executionInfo":{"status":"aborted","timestamp":1744137897278,"user_tz":360,"elapsed":142800,"user":{"displayName":"CS711 711 BETA","userId":"11837583817338767600"}}},"outputs":[],"source":["# Install Google API client library (only needed first time)\n","!pip install google-api-python-client\n","\n","from google.colab import auth\n","from googleapiclient.discovery import build\n","import json\n","\n","# ----------------------------\n","# Google Drive Authentication\n","# ----------------------------\n","def authenticate_drive():\n","    \"\"\"Authenticates the user and sets up Google Drive API service.\"\"\"\n","    auth.authenticate_user()  # Triggers Google authentication flow\n","    return build('drive', 'v3')  # Returns Drive API service instance\n","\n","# Initialize Drive service\n","drive_service = authenticate_drive()\n","\n","# ----------------------------\n","# Configuration\n","# ----------------------------\n","# File ID extracted from Google Drive shareable link\n","POSE_FEEDBACK_FILE_ID = \"1F7NEMAqa4a63P53JPRTmaYeI1hkTFvWQ\"\n","\n","# ----------------------------\n","# Data Loading Functions\n","# ----------------------------\n","def load_feedback_file(service, file_id):\n","    \"\"\"\n","    Loads JSON feedback file from Google Drive.\n","\n","    Args:\n","        service: Authenticated Drive API service\n","        file_id: Google Drive file ID\n","\n","    Returns:\n","        dict: Parsed JSON content\n","    \"\"\"\n","    try:\n","        request = service.files().get_media(fileId=file_id)\n","        json_content = request.execute().decode(\"utf-8\")\n","        return json.loads(json_content)\n","    except Exception as e:\n","        print(f\"Error loading feedback file: {str(e)}\")\n","        return {}\n","\n","# Load pose feedback data\n","pose_feedback = load_feedback_file(drive_service, POSE_FEEDBACK_FILE_ID)\n","\n","# ----------------------------\n","# Feedback Retrieval\n","# ----------------------------\n","def get_pose_feedback(pose_name):\n","    \"\"\"\n","    Retrieves feedback for a specific yoga pose.\n","\n","    Args:\n","        pose_name (str): Name of the yoga pose\n","\n","    Returns:\n","        str: Formatted feedback message or default not found message\n","\n","    Example:\n","        >>> get_pose_feedback(\"Tree Pose\")\n","        \"For Tree Pose: Keep your spine straight and focus on a fixed point...\"\n","    \"\"\"\n","    feedback = pose_feedback.get(pose_name)\n","\n","    if feedback:\n","        return f\"For {pose_name}:\\n{feedback}\"\n","    else:\n","        return f\"No specific feedback available for {pose_name}. Focus on proper alignment and breathing.\"\n"]},{"cell_type":"markdown","metadata":{"id":"eBKGfGMxqafS"},"source":["# Testing by Image Upload"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PtraQ1HnuQts","executionInfo":{"status":"aborted","timestamp":1744137897279,"user_tz":360,"elapsed":142801,"user":{"displayName":"CS711 711 BETA","userId":"11837583817338767600"}}},"outputs":[],"source":["import json\n","from google.colab import files\n","import os\n","\n","# ----------------------------\n","# Image Upload and Validation\n","# ----------------------------\n","print(\"Please upload a yoga pose image for analysis...\")\n","uploaded = files.upload()  # Triggers file upload dialog\n","\n","# Get the first uploaded filename\n","test_image = list(uploaded.keys())[0]\n","print(f\"\\nUploaded image: {test_image}\")\n","\n","# Validate the file exists\n","if not os.path.exists(test_image):\n","    print(f\"\\nError: The uploaded image '{test_image}' could not be found.\")\n","    exit()\n","\n","# ----------------------------\n","# Pose Prediction\n","# ----------------------------\n","# Get reference keypoints from your dataset\n","reference_keypoints = pose_data\n","\n","# Make prediction\n","try:\n","    predicted_pose, keypoints = predict_pose(test_image)\n","    print(f\"\\nModel prediction: {predicted_pose}\")\n","\n","    # Visualize the pose with reference comparison\n","    visualize_pose_static(\n","        test_image,\n","        keypoints,\n","        reference_keypoints,\n","        predicted_pose,\n","        'Beginner'  # Default level\n","    )\n","\n","except Exception as e:\n","    print(f\"\\nError during pose prediction: {str(e)}\")\n","    exit()\n","\n","# ----------------------------\n","# Feedback Retrieval\n","# ----------------------------\n","try:\n","    feedback = get_pose_feedback(predicted_pose)\n","\n","    print(\"\\n\" + \"=\"*50)  # Visual separator\n","\n","    # Enhanced feedback display\n","    if isinstance(feedback, dict):\n","        print(\"\\nPOSE ANALYSIS REPORT:\")\n","        print(f\"\\nPose: {predicted_pose}\")\n","\n","        print(\"\\nADVANTAGES:\")\n","        print(\"- \" + \"\\n- \".join(feedback.get(\"advantages\", [\"No specific advantages noted\"])))\n","\n","        print(\"\\nRISKS:\")\n","        print(\"- \" + \"\\n- \".join(feedback.get(\"risks\", [\"No specific risks noted\"])))\n","\n","        if \"tips\" in feedback:\n","            print(\"\\nPRO TIPS:\")\n","            print(\"- \" + \"\\n- \".join(feedback[\"tips\"]))\n","    else:\n","        print(feedback)  # Fallback for string feedback\n","\n","except Exception as e:\n","    print(f\"\\nError retrieving feedback: {str(e)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3-6EG2LinWWN","executionInfo":{"status":"aborted","timestamp":1744137897279,"user_tz":360,"elapsed":142801,"user":{"displayName":"CS711 711 BETA","userId":"11837583817338767600"}}},"outputs":[],"source":["import pickle\n","with open('yoga_pose_detection.pickle','wb') as f:\n","  pickle.dump(clf,f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LGFQY69iidid","executionInfo":{"status":"aborted","timestamp":1744137897279,"user_tz":360,"elapsed":142801,"user":{"displayName":"CS711 711 BETA","userId":"11837583817338767600"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"V28"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":0}